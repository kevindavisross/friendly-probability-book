---
title: "Untitled"
format: html
---


## covariance

Quantities like long run average, variance, and standard deviation summarize characteristics of the *marginal* distribution of a single random variable.
When there are multiple random variables their *joint* distribution is of interest.
Covariance and correlation each summarize in a single number a characteristic of the joint distribution of two random variables, namely, the degree to which they "co-deviate from the their respective means".

We'll illustrate the computations using the Bivariate Normal model of arrival times in the meeting problem from @sec-language-simulation-computer-meeting-bvn.
Covariance is the average of the paired deviations from the respective means.
Let $T$ and $W$ be the first arrival time and waiting time random variables from the meeting problem in the previous section. 
Recall that we have already simulated some $(T, W)$ pairs

```{python}
t_and_w = meeting_sim[2, 3]
```

The simulated long run averages are (notice that calling `mean` returns the pair of averages)

```{python}
t_and_w.mean()
```

Now within each simulated $(t, w)$ pair, we subtract the mean of the $T$ values from $t$ and the mean of the $W$ values from $w$,
We can do this subtract in a "vectorized" way: the following code subtracts the first component of `t_and_w.mean()` (the average of the $T$) from the first component of each of the `t_and_w` values (the individual $t$ value), and similarly for the second.

```{python}
t_and_w - t_and_w.mean()
```


For each simulated pair, we now have a pair of deviations from the respective mean.
To measure the interaction between these deviations, we take the *product* of the deviations within each pair.
First, we split the paired deviations into two separate columns and then take the product within each row

```{python}
t_deviations = (t_and_w - t_and_w.mean())[0] # first column (Python 0)
w_deviations = (t_and_w - t_and_w.mean())[1] # second column (Python 1)

t_deviations * w_deviations

```


Now we take the average of the products of the paired deviations.

```{python}
(t_deviations * w_deviations).mean()
```

This value is the *covariance*, which we could have arrived at more quickly by calling `cov` on the simulated pairs.

```{python}
t_and_w.cov()

```

The **covariance** of random variables $X$ and $Y$ is defined as the long run average of the product of the paired deviations from the respective means

$$
\text{Covariance($X$, $Y$)} = \text{Average of} ((X - \text{Average of }X)(Y - \text{Average of }Y))
$$
It turns out that covariance is equivalent to the average of the product minus the product of the averages.

$$
\text{Covariance($X$, $Y$)} = \text{Average of} XY  - (\text{Average of }X)(\text{Average of }Y)
$$

```{python}
(t_and_w[0] * t_and_w[1]).mean() - (t_and_w[0].mean()) * (t_and_w[1].mean()) 
```

Unfortunately, the value of covariance is hard to interpret as it depends heavily on the measurement unit scale of the random variables.
In the meeting problem, covariance is measured in squared-minutes. If we converted the $T$ values from minutes to seconds, then the value of covariance would change accordingly and would be measured in (minutes $\times$ seconds).

But the sign of the covariance does have practical meaning.

-   A positive covariance indicate an overall *positive association*: above average values of $X$ tend to be associated with above average values of $Y$
-   A negative covariance indicates am overall *negative association*: above average values of $X$ tend to be associated with below average values of $Y$
-   A covariance of zero indicates that the random variables are  **uncorrelated**: there is no overall positive or negative association.
But be careful: if $X$ and $Y$ are  uncorrelated there can still be a relationship between $X$ and $Y$.
We will see examples later that demonstrate that being uncorrelated does not necessarily imply that random variables are independent.


:::: {.callout-note appearance="simple"}
::: {#exm-dice-covariance}
Consider the probability space corresponding to two rolls of a fair four-sided die.
Let $X$ be the sum of the two rolls, $Y$ the larger of the two rolls, $W$ the number of rolls equal to 4, and $Z$ the number of rolls equal to 1.
Without doing any calculations, determine if the covariance between each of the following pairs of variables is positive, negative, or zero.
Explain your reasoning conceptually.

1.  $X$ and $Y$
1.  $X$ and $W$ 
1.  $X$ and $Z$ 
1.  $X$ and $V$, where $V = W + Z$.
1.  $W$ and $Z$

:::
::::

:::: {.callout-tip collapse=true}
::: {#sol-dice-covariance}


1.  Positive. There is an overall positive association; above average values of $X$ tend to be associated with above average values of $Y$ (e.g., (7, 4), (8, 4)), and below average values of $X$ tend to be associated with below average values of $Y$ (e.g., (2, 1), (3, 2)).
1.  Positive.  If $W$ is large (roll many 4s) then $X$ (sum) tends to be large.
1.  Negative.  If $Z$ is large (roll many 1s) then $X$ (sum) tends to be small.
1.  Zero.. Basically, the positive association between $X$ and $W$ cancels out with the negative association of $X$ and $Z$.  $V$ is large when there are many 1s or many 4s, or some mixture of 1s and 4s.
So knowing that W is large doesn't really tell you anything about the sum.
1.  Negative.  There is a fixed number of rolls.  If you roll lots of 4s ($W$ is large) then there must be few rolls of 1s ($Z$ is small).

:::
::::





The numerical value of the covariance depends on the measurement units of both variables, so interpreting it can be difficult.
Covariance is a measure of joint association between two
random variables that has many nice theoretical properties, but the *correlation (coefficient)* is often a more practical measure.
(We saw a similar idea with variance and standard deviation. Variance has many nice theoretical properties.
However, standard
deviation is often a better practical measure of variability.)



  
The correlation for two random variables is the covariance between the corresponding standardized random variables.  Therefore, correlation is a *standardized* measure of the association between two random variables.

Returning to the $(T, W)$ example, we carry out the same process, but first standardize each of the simulated values of $T$ by subtracting their mean and dividing by their standardization, and similarly for $W$.

```{python}
t_and_w.standardize()
```

Now we compute the covariance of the standardized values.

```{python}
t_and_w.standardize().cov()
```

Recall that we already computed the correlation in the previous section.

```{python}
t_and_w.standardize().corr()
```


We see that the correlation is the covariance of the standardized random variables.

$$
\text{Correlation}(X, Y) = \text{Covariance}\left(\frac{X- \text{Average of }X}{\text{Standard Deviation of }X}, \frac{Y- \text{Average of }Y}{\text{Standard Deviation of }Y}\right)
$$


When standardizing, subtracting the means doesn't change the scale of the possible pairs of values; it merely shifts the center of the joint distribution.
Therefore, **correlation** is the covariance divided by the product of the standard deviations.

$$
\text{Correlation}(X, Y) = \frac{\text{Covariance}(X, Y)}{(\text{Standard Deviation of }X)(\text{Standard Deviation of }Y)}
$$


A correlation coefficient has no units and is measured on a universal scale.
Regardless of the original measurement units of the random variables $X$ and $Y$
$$
-1\le \textrm{Correlation}(X,Y)\le 1
$$

- $\textrm{Correlation}(X,Y) = 1$ if and only if $Y=aX+b$ for some $a>0$
- $\textrm{Correlation}(X,Y) = -1$ if and only if $Y=aX+b$ for some $a<0$


Therefore, correlation is a standardized measure of the strength of the *linear* association between two random variables.
The closer the correlation is to 1 or $-1$, the closer the joint distribution of $(X, Y)$ pairs hugs a straight line, with positive or negative slope.



Because correlation is computed between standardized random variables,  correlation is not affected by a linear rescaling of either variable.
A value that is "one standard deviation above the mean" is "one standard deviation above the mean", whether the variable measured in feet or inches or meters.



### Joint Normal distributions {#sec-sim-bvn}

Just as Normal distributions are commonly assumed for marginal distributions of individual random variables, joint Normal distributions are often assumed for joint distributions of several random variables.
We'll focus "Bivariate Normal" distributions which describe a specific pattern of joint variability for *two* random variables.

:::: {.callout-note appearance="simple"}
::: {#exm-dd-bvn-spinner}

Donny Don't has just completed a problem where it was assumed that SAT Math scores follow a Normal(500, 100) distribution. Now a follow up problem asks Donny how he could simulate a single (Math, Reading) pair of scores. Donny says: "That's easy; just spin the Normal(500, 100) twice, once for Math and once for Reading."
Do you agree?
Explain your reasoning.

:::
::::

:::: {.callout-tip collapse=true}
::: {#sol-dd-bvn-spinner}

You should not agree with Donny, for two reasons.

-   It's possible that the distribution of SAT Math scores follow a different pattern than SAT Reading scores.  So we might need one spinner to simulate a Math score, and a second spinner to simulate the Reading score.  (In reality, SAT Math and Reading scores do follow pretty similar distributions.  But it's possible that they could follow different distributions.)
-   Furthermore, there is probably some relationship between scores.  It is plausible that students who do well on one test tend to do well on the other.  For example, students who score over 700 on Math are probably more likely to score above than below average on Reading.  If we simulate a pair of scores by spinning one spinner for Math and a separate spinner for Reading, then there will be no relationship between the scores because the spins are physically independent.

:::
::::

In the previous problem, hat we really need is a spinner that generates a pair of scores simultaneously to reflect their association.
This is a little harder to visualize, but we could imagine spinning a "globe" with lines of latitude corresponding to SAT Math score and lines of longitutde to SAT Reading score.
But this would not be a typical globe:

-   The lines of latitude would not be equally spaced, since SAT Math scores are not equally likely.  (We have seen similar issues for one-dimensional Normal spinners.) Similarly for lines of longitude.
-   The scale of the lines of latitude would not necessarily match the scale of the lines of longitude, since Math and Reading scores could follow difference distributions.  For example, the equator (average Math) might be 500 while the prime meridian (average Reading) might be 520.
-   The "lines" would be tilted or squiggled to reflect the relationship between the scores.  For example, the region corresponding to Math scores near 700 and Reading scores near 700 would be larger than the region corresponding to Math scores near 700 but Reading scores near 200. 

So we would like a model that

-   Simulates Math scores that follow a Normal distribution pattern, with some mean and some standard deviation.
-   Simulates Reading scores that follow a Normal distribution pattern, with possibly a different mean and standard deviation.
-   Reflects how strongly the scores are associated.

Such a model is called a "Bivariate Normal" distribution.
There are five parameters: the two means, the two standard deviations, and the *correlation* which reflects the strength of the association between the two scores.

We have already encountered a Bivariate Normal model.
Recall that in one case of the meeting problem we assumes that Regina and Cady tend to arrive around the same time.
One way to model pairs of values that have correlation is with a BivariateNormal distribution, like in the following.
We assumed a Bivariate Normal distribution for the $(R, Y)$ pairs, with mean (30, 30), standard deviation (10, 10) and correlation 0.7.
(We could have assumed different means and standard deviations.)


```{python}
#| eval: false
R, Y = RV(BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7))

(R & Y).sim(1000).plot()
```

```{python}
#| echo: false
R, Y = RV(BivariateNormal(mean1 = 30, sd1 = 10, mean2 = 30, sd2 = 10, corr = 0.7))

plt.figure()
(R & Y).sim(1000).plot()
plt.show()
```

Now we see that the points tend to follow the $R = Y$ line, so that Regina and Cady tend to arrive near the same time, similar to @fig-meeting-probmeasure3.


Recall that in some of the previous examples the shapes of one-dimensional histograms could be approximated with a smooth density curve.
Similarly, a two-dimensional histogram can sometimes be approximated with a smooth density surface.
As with histograms, the height of the density surface at a particular $(X, Y)$ pair of values can be represented by color intensity.  A marginal Normal distribution is a "bell-shaped curve"; a Bivariate Normal distribution is a "mound-shaped" curve --- imagine a pile of sand.
(Symbulate does not yet have the capability to display densities in a three-dimensional-like plot such as [this plot](https://en.wikipedia.org/wiki/Multivariate_normal_distribution#/media/File:Multivariate_Gaussian.png).)

```{r}
#| echo: false
#| warning: false

unit_grid = seq(0, 1, 0.001) * 60

unit_box <- expand_grid(x = unit_grid, y = unit_grid)

waiting_corr = 0.7
waiting_sd = 10
waiting_mean = 30

unit_box_prob <- unit_box |>
  mutate(independent_uniform = (1 / 60) ^ 2,
         independent_normal = dnorm(x, waiting_mean, waiting_sd) * dnorm(y, waiting_mean, waiting_sd),
         bivariate_normal = dbvn(x, y, waiting_mean, waiting_mean, waiting_sd, waiting_sd, waiting_corr))

unit_box_max_prob = unit_box_prob |>
  dplyr::select(!(1:2)) |>
  pivot_longer(cols = everything()) |>
  summarize(max(value)) |>
  pull()

```

```{r}
#| label: fig-meeting-probmeasure33
#| echo: false
#| warning: false
#| message: false
#| fig-cap: "A Bivariate Normal distribution"


ggplot(unit_box_prob,
       aes(x = x,
           y = y,
           z = bivariate_normal)) +
  geom_raster(aes(fill = bivariate_normal), interpolate = TRUE) +
  geom_contour(color = "white") +
  scale_x_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_y_continuous(expand = c(0, 0), limits = c(0, 60)) +
  scale_fill_viridis(limits = c(0, unit_box_max_prob)) +
  labs(x = "Regina's arrival time (minutes after noon)",
       y = "Cady's arrival time (minutes after noon)",
       fill = "Density") +
  coord_fixed() +
  theme_classic()

```


## conditional average


Section here about how all things can change with conditioning: EV, SD, correlation

Conditioning on the value of a random variable $X$ in general changes the distribution of another random variable $Y$.  If a distribution changes, its summary characteristics like expected value and variance can change too.


:::: {.callout-note appearance="simple"}
::: {#exm-dice-ce}
Roll a fair four-sided die twice.  Let $X$ be the sum of the two rolls, and let $Y$ be the larger of the two rolls (or the common value if a tie). We found the joint and marginal distributions of $X$ and $Y$ in Example \@ref(exm:dice-probspace), displayed in the table below.


| $p_{X, Y}(x, y)$ |      |      |      |      |            |
|------------------|-----:|-----:|-----:|-----:|-----------:|
| $x$ \\ $y$       |    1 |    2 |    3 |    4 | $p_{X}(x)$ |
| 2                | 1/16 |    0 |    0 |    0 |       1/16 |
| 3                |    0 | 2/16 |    0 |    0 |       2/16 |
| 4                |    0 | 1/16 | 2/16 |    0 |       3/16 |
| 5                |    0 |    0 | 2/16 | 2/16 |       4/16 |
| 6                |    0 |    0 | 1/16 | 2/16 |       3/16 |
| 7                |    0 |    0 |    0 | 2/16 |       2/16 |
| 8                |    0 |    0 |    0 | 1/16 |       1/16 |
| $p_Y(y)$         | 1/16 | 3/16 | 5/16 | 7/16 |            |


1.  Find $\E(Y)$. How could you find a simulation-based approximation?
1.  Find $\E(Y|X=6)$.  How could you find a simulation-based approximation?
1.  Find $\E(Y|X=5)$.  How could you find a simulation-based approximation?
1.  Find $\E(Y|X=x)$ for each possible value of $x$ of $X$.
1.  Find $\E(X|Y = 4)$. How could you find a simulation-based approximation?
1.  Find $\E(X|Y = y)$ for each possible value $y$ of $Y$.

:::
::::



:::: {.callout-tip collapse=true}
::: {#sol-dice-ce}


1.  $\E(Y) = 1(1/16) + 2(3/16) + 3(5/16) + 4(7/16) = 3.125$.  Approximate this long run average value by simulating many values of $Y$ and computing the average.
1.  The conditional pmf of $Y$ given $X=6$ places probability 2/3 on the value 4 and 1/3 on the value 3.  Compute the expected value using this conditional distribution: $\E(Y|X=6) = 3(1/3) + 4(2/3) = 3.67$.  This conditional long run average value could be approximated by simulating many $(X, Y)$ pairs from the joint distribution, discarding the pairs for which $X\neq 6$, and computing the average value of the $Y$ values for the remaining pairs.
1.  The conditional pmf of $Y$ given $X=5$ places probability 1/2 on the value 4 and 1/2 on the value 3.  Compute the expected value using this conditional distribution: $\E(Y|X=5) = 3(1/2) + 4(1/2) = 3.5$.  This conditional long run average value could be approximated by simulating many $(X, Y)$ pairs from the joint distribution, discarding the pairs for which $X\neq 5$, and computing the average value of the $Y$ values for the remaining pairs.
1.  Proceed as in the previous two parts. Find $\E(Y|X=x)$ for each possible value of $x$ of $X$.

    | $x$ |            $\E(Y|X=x)$ |
    |----:|-----------------------:|
    |   2 |               1(1) = 1 |
    |   3 |               2(1) = 2 |
    |   4 |  2(1/3) + 3(2/3) = 8/3 |
    |   5 |  3(1/2) + 4(1/2) = 3.5 |
    |   6 | 3(1/3) + 4(2/3) = 11/3 |
    |   7 |               4(1) = 4 |
    |   8 |               4(1) = 4 |
  
1.  The conditional pmf of $X$ given $Y=4$ places probability 2/7 on each of the values 5, 6, 7, and 1/7 on the value 8.  Compute the expected value using this conditional distribution: $\E(X|Y=4) = 5(2/7) + 6(2/7) +7(2/7) + 8(1/7)= 6.29$.  This conditional long run average value could be approximated by simulating many $(X, Y)$ pairs from the joint distribution, discarding the pairs for which $Y\neq 4$, and computing the average value of the $X$ values for the remaining pairs.
1.  Proceed as in the previous part.

    | $y$ |                             $\E(X|Y=y)$ |
    |----:|----------------------------------------:|
    |   1 |                                2(1) = 2 |
    |   2 |                  3(2/3) + 4(1/3) = 10/3 |
    |   3 |          4(2/5) + 5(2/5) + 6(1/5) = 4.8 |
    |   4 | 5(2/7) + 6(2/7) +7(2/7) + 8(1/7) = 44/7 |

:::
::::


```{python}

U1, U2 = RV(DiscreteUniform(1, 4) ** 2)

X = U1 + U2

Y = (U1 & U2).apply(max)

y_given_Xeq6 = (Y | (X == 6) ).sim(3000)

y_given_Xeq6

```


```{python}

y_given_Xeq6.tabulate()

```

```{python}

y_given_Xeq6.mean()

```



::: {.definition #ce}

The **conditional expected value** (a.k.a. *conditional expectation* a.k.a. *conditional mean*), of a random variable $Y$ given the event $\{X=x\}$, defined on a probability space with measure $\IP$, is a *number* denoted $\E(Y|X=x)$ representing the probability-weighted average value of $Y$, where the weights are determined by the conditional distribution of $Y$ given $X=x$.

\begin{align*}
	& \text{Discrete $X, Y$ with conditional pmf $p_{Y|X}$:} & \E(Y|X=x) & = \sum_y y p_{Y|X}(y|x)\\
	& \text{Continuous $X, Y$ with conditional pdf $f_{Y|X}$:} & \E(Y|X=x) & =\int_{-\infty}^\infty y f_{Y|X}(y|x) dy
\end{align*}

:::


Remember, when conditioning on $X=x$, $x$ is treated as a fixed constant. The conditional expected value  $\E(Y | X=x)$ is  a *number* representing the mean of the conditional distribution of $Y$ given $X=x$. The conditional expected value $\E(Y | X=x)$ is the long run average value of $Y$ over only those outcomes for which $X=x$.
To approximate $\E(Y|X = x)$, simulate many $(X, Y)$ pairs, discard the pairs for which $X\neq x$, and average the $Y$ values for the pairs that remain. 






:::: {.callout-note appearance="simple"}
::: {#exm-uniform-sum-max-ce}

Recall Example \@ref(exm:uniform-sum-max-conditional). Consider the probability space corresponding to two spins of the Uniform(1, 4) spinner and let $X$ be the sum of the two spins and $Y$ the larger to the two spins (or the common value if a tie).

1.  Find $\E(X | Y = 3)$.
1.  Find $\E(X | Y = 4)$.
1.  Find $\E(X | Y = y)$ for each value $y$ of $Y$.
1.  Find $\E(Y | X = 3.5)$.
1.  Find $\E(Y | X = 6)$.
1.  Find $\E(Y | X = x)$ for each value $x$ of $X$.

:::
::::

:::: {.callout-tip collapse=true}
::: {#sol-uniform-sum-max-ce}

1.  Recall Figure \@ref(fig:uniform-sum-max-conditional-plot).  All the conditional distributions (slices) are Uniform, but over different ranges of possible values. Remember, the mean of any Uniform distribution is the midpoint of the possible range of values. The conditional distribution of $X$ given $Y=3$ is the Uniform(4, 6) distribution, which has mean 5.  Therefore, $\E(X | Y = 3)=5$.  As an integral, since $f_{X|Y}(x|3) = 1/2, 4<x<6$,
$$
\E(X | Y = 3) = \int_4^6  x (1/2)dx = \frac{x^2}{4}\Bigg|_{x=4}^{x=6} = \frac{6^2}{4} - \frac{4^2}{4} = 5.
$$
1.  The conditional distribution of $X$ given $Y=4$ is the Uniform(5, 8) distribution, which has mean 6.5.  Therefore, $\E(X | Y = 4)=6.5$.
1.  For a given $y$, the conditional distribution of $X$ given $Y=y$ is the Uniform($y+1$, $2y$) distribution, which has mean $\frac{y+1+2y}{2}=1.5y + 0.5$.  Therefore, $\E(X | Y = y)=1.5y + 0.5$. As an integral, since $f_{X|Y}(x|y) = \frac{1}{y-1}, y+1 < x< 2y$,
$$
\E(X | Y = y) = \int_{y+1}^{2y}  x \left(\frac{1}{y-1}\right)dx = \frac{x^2}{2(y-1)}\Bigg|_{x=y+1}^{x=2y} = \frac{(2y)^2}{2(y-1)} - \frac{(y+1)^2}{2(y-1)} = 1.5y + 0.5.
$$
In the above, $y$ is treated like a fixed constant, and we are averaging over values of $X$ by taking a $dx$ integral. For a given value of $y$ like $y=3$, $1.5y + 0.5$ is a number like $1.5(3)+0.5=5$.
1.  The conditional distribution of $Y$ given $X=3.5$ is the Uniform(1.75, 2.5) distribution, which has mean 2.125.  Therefore, $\E(Y | X = 3.5)=2.125$.  As an integral, since $f_{Y|X}(y|3.5) = 1/0.75, 1.75<y<2.5$,
$$
\E(Y | X = 3.5) = \int_{1.75}^{2.5}  y (1/0.75)dy = \frac{y^2}{1.5}\Bigg|_{y=1.75}^{y=2.5} = \frac{2.5^2}{1.5} - \frac{1.75^2}{1.5} = 2.125.
$$
1.  The conditional distribution of $Y$ given $X=6$ is the Uniform(3, 4) distribution, which has mean 3.5.  Therefore, $\E(Y | X = 6)=3.5$.
1.  There are two general cases.  If $2<x<5$ then the conditional distribution of $Y$ given $X=x$ is Uniform($0.5x$, $x-1$) so $\E(Y|X = x) = \frac{0.5x + x - 1}{2} = 0.75x - 0.5$. If $5<x<8$ then the conditional distribution of $Y$ given $X=x$ is Uniform($0.5x$, 4) so $\E(Y|X = x) = \frac{0.5x + 4}{2} = 0.25x +2$.  The two cases can be put together as
$$
\E(Y | X = x) = 0.25x + 0.5\min(4, x-1).  
$$
In the above, $x$ is treated like a fixed constant, and we are averaging over values of $Y$ by taking a $dy$ integral. For a given value of $x$ like $x=3.5$, $0.25x + 0.5\min(4, x-1)$ is a number like $0.25(3.5) + 0.5\min(4, 3.5-1)=2.125$.
    
:::
::::


Remember that the probability that a continuous random variable is equal to a particular value is 0; that is, for continuous $X$, $\IP(X=x)=0$. When we condition on $\{X=x\}$ we are really conditioning on $\{|X-x|<\ep\}$ and seeing what happens in the idealized limit when $\ep\to0$.

When simulating, *never* condition on $\{X=x\}$ for a continuous random variable $X$; rather, condition on $\{|X-x|<\ep\}$ where $\ep$ represents some suitable degree of precision (e.g. $\ep=0.005$ if rounding to two decimal places).

To approximate $\E(Y|X = x)$ for continuous random variables, simulate many $(X, Y)$ pairs, discard the pairs for which $X$ *is not close to* $x$, and average the $Y$ values for the pairs that remain. 

```{python}

U1, U2 = RV(Uniform(1, 4) ** 2)

X = U1 + U2

Y = (U1 & U2).apply(max)

x_given_Yeq3 = (X | (abs(Y - 3) < 0.05) ).sim(1000)

x_given_Yeq3

```



```{python}

x_given_Yeq3.plot()
plt.show()

x_given_Yeq3.mean()

```




## Simulating from distributions



## Do not confuse a random variable with its distribution {#sec-rv-versus-distribution}

