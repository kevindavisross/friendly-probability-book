# Chapter 2 Exercise Solutions

{{< include _r_setup.qmd >}}

{{< include _python_setup.qmd >}}

::: {.content-hidden}
$$
{{< include _macros.tex >}}
$$
:::


## Solution to @exr-collector4-outcome


1.  There are 3 possible prizes in each of 3 boxes, so there are $3^3 = 27$ possible outcomes.
1.  See @tbl-collector3 (ignore $X$ and $Y$ columns for now); there are 27 rows, each row for a different possible outcome.

```{r}
#| label: tbl-collector3
#| tbl-cap: "Sample space for @exr-collector4-outcome"
#| echo: false
#| warning: false

k = 3

outcomes = expand.grid(1:k, 1:k, 1:k)


n_outcomes = nrow(outcomes)

X = rep(NA, n_outcomes)
Y = rep(NA, n_outcomes)

for (i in 1:n_outcomes) {
  X[i] = outcomes %>% slice(i) %>% unlist() %>% n_distinct()
  Y[i] = sum(outcomes[i, ] == 1)
}

outcomes <- outcomes %>%
  bind_cols(X, Y)

names(outcomes) = c(paste("box", 1:k, sep = ""), "X", "Y")

outcomes %>%
  kbl() %>%
  kable_styling(fixed_thead = TRUE)

```





## Solution to @exr-event-collector3

See the sample space $\Omega$ of 27 possible outcomes in @tbl-collector3.


1.  $A_1^c = \{222, 223, 232, 322, 233, 323, 332, 333\}$ is the event that none of the boxes contain prize 1, so $A_1$ consists of the $27-8 = 19$ other outcomes.
1.  $B_1 = \{111\}$
1.  $A_1 \cap A_2 \cap A_3 = \{123, 132, 213, 231, 312, 321\}$ is the event that at least one of each prize is obtained (that is, a complete set of prizes)
1.  $A_1 \cup A_2 \cup A_3 = \Omega$, the set of all possible outcomes; you have to get at least 1 of one of the prizes.
1.  $B_1 \cap B_2 \cap B_3=\emptyset$; you can't get only prize 1 and only prize 2
1.  $B_1 \cup B_2 \cup B_3 = \{111, 222, 333\}$ is the event you only obtain one of the prizes (in every box)




## Solution to @exr-event-dartboard 

See @fig-event-dartboard

1.  @fig-event-dartboard-1: $A$, Katniss's dart lands within 1 inch of the center of the dartboard.
1.  @fig-event-dartboard-2: $B$, Katniss's dart lands more than 1 inch but less than 2 inches away from the center of the dartboard.
1.  @fig-event-dartboard-3: $E$, Katniss's dart lands within 1 inch of the outside edge of the dartboard.


```{r}
#| label: fig-event-dartboard
#| echo: false
#| fig-cap: "Events in @exr-event-dartboard"
#| fig-subcap: 
#|   - "Within 1 inch of center"
#|   - "More than 1 but less than 2 inches from center"
#|   - "Within 1 inch of edge"
#| layout-ncol: 3


# Draw 12 equally spaced annuli with the outermost shaded and a center dot

circle_xy <- function(cx = 0, cy = 0, r = 1, n = 720) {
  t <- seq(0, 2*pi, length.out = n)
  cbind(x = cx + r*cos(t), y = cy + r*sin(t))
}

R <- 1
n_annuli <- 12
radii <- seq(0, R, length.out = n_annuli + 1)

# INNER 
par(mar = c(1,1,1,1))
plot(0, 0, type = "n", asp = 1, xlab = "", ylab = "",
     xlim = c(-1.05, 1.05), ylim = c(-1.05, 1.05), axes = FALSE, bty = "n")


# --- Shade inner annulus (just a filled circle of radius 1/12) ---
inner <- circle_xy(r = radii[2])   # radius = 1/12
polygon(inner, col = "orange", border = NA)

# Draw annulus boundaries
for (r in radii[-1]) {
  boundary <- circle_xy(r = r)
  lines(boundary, lwd = 1.2)
}

# Add center dot
points(0, 0, pch = 19, cex = 0.5)


# SECOND INNER

par(mar = c(1,1,1,1))
plot(0, 0, type = "n", asp = 1, xlab = "", ylab = "",
     xlim = c(-1.05, 1.05), ylim = c(-1.05, 1.05), axes = FALSE, bty = "n")

# --- Shade the 2nd innermost annulus (between r=1/12 and r=2/12) ---
outer_ring <- circle_xy(r = radii[3])  # radius = 2/12
polygon(outer_ring, col = "orange", border = NA)
inner_hole <- circle_xy(r = radii[2])  # radius = 1/12
polygon(inner_hole, col = "white", border = NA)

# Draw annulus boundaries
for (r in radii[-1]) {
  boundary <- circle_xy(r = r)
  lines(boundary, lwd = 1.2)
}

# Add center dot
points(0, 0, pch = 19, cex = 0.5)


# OUTER

par(mar = c(1,1,1,1))
plot(0, 0, type = "n", asp = 1, xlab = "", ylab = "",
     xlim = c(-1.05, 1.05), ylim = c(-1.05, 1.05), axes = FALSE, bty = "n")

# Shade the outer annulus
outer <- circle_xy(r = R)
polygon(outer, col = "orange", border = NA)
inner_hole <- circle_xy(r = radii[n_annuli])
polygon(inner_hole, col = "white", border = NA)

# Draw annulus boundaries
for (r in radii[-1]) {
  boundary <- circle_xy(r = r)
  lines(boundary, lwd = 1.2)
}

# Add center dot
points(0, 0, pch = 19, cex = 0.5)




```


## Solution to @exr-rv-collector3





1.  See @tbl-collector3
1.  The possible values of $X$ are $\{1, 2, 3\}$
1.  The possible values of $Y$ are $\{0, 1, 2, 3\}$
1.  The possible $(X, Y)$ pairs are $\{(1, 0), (1, 3), (2, 0), (2, 1) (2, 2), (3, 1)\}$,
In particular, the following pairs are NOT possible: $(1, 1), (1, 2), (2, 3), (3, 0), (3, 2), (3, 3)$
1.  $\{X = 1\} = \{111, 222, 333\}$ is the event that only one distinct prize is obtained (that is, you get the same prize in every box) 
1.  $\{X=2\}$ is the event you get two distinct prizes, which consists of 18 putcomes.
It's easier to write $\{X = 2\}^c = \{111, 222, 333, 123, 132, 213, 231, 312, 321\}$.
1.  $\{X = 3\} = \{123, 132, 213, 231, 312, 321\}$ is the event that you get all 3 prizes; that is, the event that you get the complete set
1.  $\{Y = 0\}=\{222, 223, 232, 322, 233, 323, 332, 333\}$ is the event that none of the boxes contain prize 1
1.  $\{Y = 1\}=\{122, 123, 132, 133, 212, 213, 312, 313, 221, 231, 321, 331\}$ is the event that exactly one of the boxes contains prize 1
1.  $\{Y = 2\}=\{112, 113, 121, 131, 211, 311\}$ is the event that exactly two of the boxes contain prize 1.
1.  $\{Y = 3\}=\{111\}$ is the event that all three of the boxes contain prize 1.
1.  $\{X = 2, Y = 1\} = \{122, 133, 212, 313, 221, 331\}$ is the event that one box contains prize one and the other two boxes contain either both prize 2 or both prize 3.
1.  $\{X = Y\} = \{112, 113, 121, 131, 211, 311\}$ is the event that the number of boxes that contain prize 1 is equal to the number of distinct prizes obtained (in this case it only happens if both $X$ and $Y$ equal 2)
1.  Let $I_1$ be the indicator random variable that prize 1 is obtained (in at least one of the three packages).
Identify and intepret $\{I_1 = 0\}$.
1.  $X = I_1+ I_2+ I_3$
1.  Label the boxes instead of the prizes.
Let $J_1$ be the indicator random variable that box 1 contains prize 1, $J_2$ that box 2 contains prize 1, and $J_3$ that box 3 contains prize 1.
Then $Y = J_1+ J_2+ J_3$.







## Solution to @exr-rv-dartboard

1.  @fig-event-dartboard-1: $\{X \le 1\}$, Katniss's dart lands within 1 inch of the center of the dartboard.
1.  @fig-event-dartboard-2: $\{1 < X < 2\}$, Katniss's dart lands more than 1 inch but less than 2 inches away from the center of the dartboard.
1.  @fig-event-dartboard-3: $\{X > 11\}$, Katniss's dart lands within 1 inch of the outside edge of the dartboard.
1.  $\{X = 0\}$, the event that the dart hits exactly in the center, is just the single point in the center
1.  $\{X = 1\}$, the event that the dart hits exactly 1 inch from the center, is the circle with radius 1 inch (the *outside egde* of the shaded region in @fig-event-dartboard-1)






## Solution to @exr-probspace-collector3-a

The latest series of collectible Lego Minifigures contains 3 different Minifigure prizes (labeled 1, 2, 3).
Each package contains a single unknown prize.
Suppose we only buy 3 packages and we consider as our sample space outcome the results of just these 3 packages (prize in package 1, prize in package 2, prize in package 3).
For example, 323 (or (3, 2, 3)) represents prize 3 in the first package, prize 2 in the second package, prize 3 in the third package.
Suppose that each package is equally likely to contain any of the 3 prizes, regardless of the contents of other packages, so that there are 27 equally likely outcomes, and let $\IP$ be the corresponding probability measure.


-   Let $A_1$ be the event that prize 1 is obtained---that is, *at least one* of the packages contains prize 1---and define $A_2, A_3$ similarly for prize 2, 3.
-   Let $B_1$ be the event that *only* prize 1 is obtained---that is, *all three* packages contain prize 1---and define $B_2, B_3$ similarly for prize 2, 3.


1.  Compute $\IP(A_1)$
1.  Compute $\IP(B_1)$
1.  Interpret the values from parts 1 and 2 as long run relative frequencies.
1.  Interpret the values from parts 1 and 2 as relative likelihoods.
1.  Compute $\IP(A_1 \cap A_2 \cap A_3)$
1.  Compute $\IP(A_1 \cup A_2 \cup A_3)$
1.  Compute $\IP(B_1 \cap B_2 \cap B_3)$
1.  Compute $\IP(B_1 \cup B_2 \cup B_3)$

1.  $A_1^c = \{222, 223, 232, 322, 233, 323, 332, 333\}$ is the event that none of the boxes contain prize 1, so $A_1$ consists of the $27-8 = 19$ other outcomes.
Since the outcomes are equally likely, $\text{P}(A_1) = 18/27=0.667$.
1.  There is only one outcome that satisfies $B_1$ so $\text{P}(B_1) = 1/27 = 0.037$.
1.  Over many sets of 3 boxes, about 66.7% of sets of 3 boxes will contain at least one prize 1, and about 3.7% of sets of 3 boxes will contain only prize 1.
1.  It is 18 times more likely to obtain at least one prize 1 than it is to obtain only prize 1.
Also, it is 2 times more likely to obtain at least one prize 1 than to not obtain it (18/9), and it is 26 times less likely to obtain only prize 1 than it is to obtain any other collection of prizes.
1.  $A_1 \cap A_2 \cap A_3 = \{123, 132, 213, 231, 312, 321\}$ is the event that at least one of each prize is obtained (that is, a complete set of prizes) so $\IP(A_1 \cap A_2 \cap A_3) = 6/27 = 0.222$
1.  $A_1 \cup A_2 \cup A_3 = \Omega$, the set of all possible outcomes; you have to get at least 1 of one of the prizes so $\IP(A_1 \cup A_2 \cup A_3) = 1$
1.  $B_1 \cap B_2 \cap B_3=\emptyset$; you can't get only prize 1 and only prize 2, so $\IP(B_1 \cap B_2 \cap B_3) = 0$
1.  $B_1 \cup B_2 \cup B_3 = \{111, 222, 333\}$ is the event you only obtain one of the prizes (in every box), so $\IP(B_1 \cup B_2 \cup B_3) = 3/27 = 0.111$





## Solution to @exr-probspace-dartboard-b

Since the dart lands uniformly at random anywhere on the dartboard, probabilities are computed as the ratio between the area corresponding to the event of interest divided by the area of the total dartboard ($12^2\pi$)

See @fig-event-dartboard.
Find the area of the shaded pieces by subtracting areas of circles.

1.  $\text{P}(X \le 1) = \frac{1^2\pi}{12^2\pi} = 1/144 = 0.00694$
1.  $\text{P}(1 < X < 2) = \frac{2^2\pi - 1^2\pi}{12^2\pi} = 3/144 = 0.021$
1.  $\text{P}(X > 11) = 1 - \frac{11^2\pi}{12^2\pi} = 1 - (11/12)^2 = 0.160$



## Solution to @exr-probspace-dartboard-c

Since the dart lands uniformly at random anywhere on the dartboard, probabilities are computed as the ratio between the area corresponding to the event of interest divided by the area of the total dartboard ($12^2\pi$)

Find the area of the events of interest by finding areas of corresponding circles and subtracting as needed.

1.  $\IP(X \le 0.1) = \frac{0.1^2\pi}{12^2\pi} = (0.1/12)^2 = 0.0000694$
1.  $\IP(X \le 0.01) = \frac{0.01^2\pi}{12^2\pi} = (0.01/12)^2 = 0.000000694$
1.  $\IP(X = 0) = 0$, the single point has no area
1.  $\IP(X \ge 11.9) = 1 - \frac{11.9^2\pi}{12^2\pi} = 1 - (11.9/12)^2 = 0.0166$
1.  $\IP(X \ge 11.99) = 1 - \frac{11.99^2\pi}{12^2\pi} = 1 - (11.99/12)^2 = 0.00166$
1.  $\IP(X = 12) = 0$, the circle representing the outside edge has no area
1.  Well, both of these events---the dart lands exactly in the center and the darts lands exactly on the edge---have 0 probability.
However for practical purposes we would never be interested in probabilities like $\IP(X = 0.000000000\ldots)$ or  $\IP(X = 12.000000000\ldots)$ with infinite precision.
1.  However we define "close to"---within 1 inch or within 0.1 inch or within 0.01 inch, etc---the dart is more likely to land close to the edge than close to the center.

## Solution to @exr-distribution-intro-collector3

1.  Construct a two-way table representing the *joint distribution* of $X$ and $Y$.
1.  Sketch a plot representing the joint distribution of $X$ and $Y$.
1.  Identify the *marginal* distribution of $X$, and sketch a plot of it.
1.  Identify the *marginal* distribution of $Y$, and sketch a plot of it.
1.  Compute and interpret $\text{E}(X)$.
1.  Compute and interpret $\text{E}(Y)$.


1.  One dimension will have possible values of $x$, the other possible values of $y$.
The interior cells should contain the probability of each $(x, y)$ pair.

    |       |      |       |      |      |       |
    |:------|-----:|------:|-----:|-----:|------:|
    |       |  $y$ |       |      |      |       |
    | $x$   |    0 |     1 |    2 |    3 | Total |
    | 1     | 2/27 |     0 |    0 | 1/27 |  3/27 |
    | 2     | 6/27 |  6/27 | 6/27 |    0 | 18/27 |
    | 3     |    0 |  6/27 |    0 |    0 |  6/27 |
    | Total | 8/27 | 12/27 | 6/27 | 1/27 |     1 |

1.  Make a tile plot with color or shading representing probability; see @fig-distribution-intro-collector3


1.  The possible values of $X$ are in the leftmost column (1, 2, 3) and the probabilities are in the Total column.
See @fig-distribution-intro-collector3-X.

1.  The possible values of $Y$ are in the top row (0, 1, 2, 3) and the probabilities are in the Total row.
See @fig-distribution-intro-collector3-Y.

1.  $\E(X) = 1\times (3/27) + 2 \times (18/27) + 3 \times (6/ 27) = 2.11$.
Over many sets of 3 boxes, we expect 2.11 distinct prizes on average.

1.  $\E(Y) = 0\times (8/27) + 1\times (12/27) + 2 \times (6/27) + 3 \times (1/ 27) = 1$.
Over many sets of 3 boxes, we expect 1 box containing prize 1 on average.


```{r}
#| echo: false
#| label: fig-distribution-intro-collector3
#| fig-cap: Joint distribution of $X$ and $Y$ in @exr-distribution-intro-collector3

x = c(1, 1, 2, 2, 2, 3)
y = c(0, 3, 0, 1, 2, 1)
p = c(2, 1, 6, 6, 6, 6) / 27


data.frame(x, y, p) |>
  ggplot(aes(x = x,
              y = y,
              fill = p)) +
  geom_tile(colour = "grey50") +
  geom_text(aes(label = round(p, 3)), col = "white") +
  scale_x_continuous(breaks = 1:3) +
  labs(fill = "Probability") +
  theme_classic()

```



```{r}
#| echo: false
#| label: fig-distribution-intro-collector3-X
#| fig-cap: Marginal distribution of $X$ in @exr-distribution-intro-collector3


ggplot(data.frame(x = 1:3, p = c(3, 18, 6)/ 27),
       aes(x = x,
           xend = x,
           y = 0,
           yend = p)) +
  geom_segment(linewidth = 1.2, col = "skyblue") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 0:3, limits = c(0, 3)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "x",
       y = "P(X = x)")

```


```{r}
#| echo: false
#| label: fig-distribution-intro-collector3-Y
#| fig-cap: Marginal distribution of $Y$ in @exr-distribution-intro-collector3


ggplot(data.frame(x = 0:3, p = c(8, 12, 6, 1)/ 27),
       aes(x = x,
           xend = x,
           y = 0,
           yend = p)) +
  geom_segment(linewidth = 1.2, col = "orange") +
  # scale_color_manual(values = c("#56B4E9", "#E69F00", "#999999")) +
  scale_x_continuous(breaks = 0:3, limits = c(0, 3)) +
  scale_y_continuous(expand = c(0, 0)) +
  theme_classic() +
  theme(legend.position = "none") +
  labs(x = "y",
       y = "P(Y = y)")

```


 
## Solution to @exr-distribution-conditional-collector3

1.  Each row of the table below represents a different conditional distribution of $Y$ given $X=x$. 
For example, the row for $x=1$ represents the conditional distribution of $Y$ given $X=1$: Given $X=1$, $Y$ is 0 with probability 2/3 and 3 with probability 1/3.

    |     |     |     |     |     |       |
    |-----|----:|----:|----:|----:|------:|
    |     | $y$ |     |     |     |       |
    |     |   0 |   1 |   2 |   3 | Total |
    | $x$ |     |     |     |     |       |
    | 1   | 2/3 |   0 |   0 | 1/3 |     1 |
    | 2   | 1/3 | 1/3 | 1/3 |   0 |     1 |
    | 3   |   0 |   1 |   0 |   0 |     1 |


1.  Take expected values according to each conditional distribution. 
In general, $\text{E}(Y|X=x)$ depends on $x$, but in this case $\text{E}(Y|X=x) = 1$ for all values of $x$.; regardless of how many distinct prizez people obtain in their, the average number of prize 1s obtains is 1.

    \begin{align*}
    x & \quad \text{E}(Y|X=x)\\
    1 & \quad 0(2/3) + 3(1/3) = 1\\
    2 & \quad 0(1/3) + 1(1/3) + 2(1/3) = 1\\
    3 & \quad 1(1) = 1
    \end{align*}

1.  Each column of the table below represents a different conditional distribution of $X$ given $Y=y$. 
For example, the column for $y=1$ represents the conditional distribution of $X$ given $Y=1$: Given $Y=1$, $X$ is 1 with probability 1/4 and 2 with probability 3/4.

    |       |     |     |     |     |
    |-------|----:|----:|----:|----:|
    |       | $y$ |     |     |     |
    |       |   0 |   1 |   2 |   3 |
    | $x$   |     |     |     |     |
    | 1     | 1/4 |   0 |   0 |   1 |
    | 2     | 3/4 | 1/2 |   1 |   0 |
    | 3     |   0 | 1/2 |   0 |   0 |
    | Total |   1 |   1 |   1 |   1 |


1.  Take expected values according to each conditional distribution. 
For example, $\text{E}(X|Y = 0) = 1.75$; among the people who never get prize 1 in their 3 boxes, the average number of distinct prizes they obtain is 1.75.

    \begin{align*}
    y & \quad \text{E}(X|Y=y)\\
    0 & \quad 1(1/4) + 2(3/4) = 1.75\\
    1 & \quad 2(1/2) + 3(1/2) = 2.5\\
    2 & \quad 2(1) = 2\\
    3 & \quad 1(1) = 1
    \end{align*}



## Solution to @exr-ltp-multiple-choice

1. Suppose there are 1000 questions on the test. (That's a long test! But remember, 1000 is just a convenient round number.) We can classify each question by its type (know, eliminate, guess) and whether we answer it correctly or not. The probability that we answer a question correctly is 1 given that we know it, 0.5 given that we can eliminate two choices, or 0.25 given that we guess randomly.

    |           | Know | Eliminate | Guess | Total |
    |-----------|-----:|----------:|------:|------:|
    | Correct   |  700 |       100 |    25 |   825 |
    | Incorrect |    0 |       100 |    75 |   175 |
    | Total     |  700 |       200 |   100 |  1000 |

    The probability that we answer a randomly selected question correctly is 825/1000 = 0.825.
    
1. The overall probability of answering a question correctly is closer to 1 than 0.5 or 0.25. To construct the table and obtain the value 0.825, we basically did the following calculation

    $$
    0.825 = (1)(0.7) + (0.5)(0.2) + (0.25)(0.1)
    $$
    
    We see that the overall probability, 0.825, is a weighted average of the case-by-case probabilities 1, 0.5, and 0.25, where 1 gets the most weight in the average because there is a higher percentage of questions that we know.


## Solution to @exr-ltp-rats

1.  Most people produce a sequence that has 30 G’s and 10 R’s, or close to those proportions, because they are trying to generate a sequence for which each outcome has a 75% chance for G and a 25% chance for R.
That is, they use a strategy in which they predict G with probability 0.75, and R with probability 0.25.
1.  There are two cases: the true flash is either green (with probability 0.75) or red (with probability 0.25). Given that the flash is green, your probability of correctly predicting it is 0.75 (because your probability of guessing "G" is 0.75).
Given that the flash is red, your probability of correctly predicting it is 0.25 (because your probability of guessing "R" is 0.25).
Use the law of total probability to find the probability that your prediction is correct: $(0.75)(0.75) + (0.25)(0.25) = 0.625$.
1.  Just pick G every time! Picking green every time has a 0.75 probability of correctly predicting any flash. When events are *independent*, trying to guess the pattern doesn't help.




## Solution to @exr-bayes-elisa

1.  We don't know what you guessed, but from experience many people guess 80-100%.
Afterall, the test is correct for most of people who carry HIV, and also correct for most people who don't carry HIV, so it seems like the test is correct most of the time.
But this argument ignores one important piece of information that has a huge impact on the results: most people do not carry HIV.
1.  Let $H$ denote the event that the person carries HIV (hypothesis), and let $E$ denote the event that the test is positive (evidence).
Therefore, $H^c$ is the event that the person does not carry HIV, another hypothesis.
We are given

-   prior probability: $P(H) = 0.005$
-   likelihood of testing positive, if the person carries HIV: $P(E|H) = 0.977$
-   $P(E^c|H^c) = 0.926$
-   likelihood of testing positive, if the person does not carry HIV: $P(E|H^c) = 1-P(E^c|H^c) = 1-0.926 = 0.074$
-   We want to find the posterior probability $P(H|E)$. 

1.  Assuming 1000000 Americans

    |                    | Tests positive | Does not test positive |   Total |
    |--------------------|---------------:|-----------------------:|--------:|
    | Carries HIV        |           4885 |                    115 |    5000 |
    | Does not carry HIV |          73630 |                 921370 |  995000 |
    | Total              |          78515 |                 921485 | 1000000 |

    Among the 78515 who test positive, 4885 carry HIV, so the probability that an American who tests positive actually carries HIV is 4885/78515 = 0.062.
1.  See @tbl-bayes-elisa.
1.  The result says that only 6.2% *of Americans who test positive* actually carry HIV.
It is true that the test is correct for most Americans with HIV (4885 out of 5000) and incorrect only for a small proportion of Americans who do not carry HIV (73630 out of 995000).
But since so few Americans carry HIV, the sheer *number* of false positives (73630) swamps the *number* of true positives (4885).
1.  Prior to observing the test result, the prior probability that an American carries HIV is $P(H) = 0.005$. 
The posterior probability that an American carries HIV given a positive test result is $P(H|E)=0.062$.
$$
  \frac{P(H|E)}{P(H)} = \frac{0.062}{0.005} =  12.44
$$
An American who tests positive is about 12.4 times more likely to carry HIV than an American whom the test result is not known.  
So while 0.067 is still small in absolute terms, the posterior probability is much larger relative to the prior probability.
1.  In this risk group

-   a person is 19 times more likely to not have HIV than to have it ($0.95/0.05 = 19$).
-   A positive test is 13.2 times *less* likely when the person does not have HIV than when they have it ($0.074/0.977 = 1/13.2$).
-   The product of these ratios is $19(1/13.2) = 1.44$.

Since posterior is proportional to the product of prior and likelihood, a person in this risk group who tests positive is 1.44 times more likely to not have HIV than to have HIV.

1.  The posterior probabilities of not having HIV and having HIV are in a 1.44 to 1 ratio, the so the posterior probability of not having HIV is $1.44/(1+1.44) = 0.59$ and the posterior probability of having HIV is 0.41.
1.  Yes, the posterior probability is influenced by the prior probability.
Even though the prior probability of 5% is still relatively small in absolute terms, the posterior probability given a positive test is not close to 50/50.


```{r}
#| echo: false
#| label: tbl-bayes-elisa
#| tbl-cap: Bayes table for @exr-bayes-elisa


hypothesis = c("Carries HIV", "Does not carry HIV")

prior = c(0.005, 1 - 0.005)

likelihood = c(0.977, 1 - 0.926) # given positive test

product = prior * likelihood

posterior = product / sum(product)

bayes_table = data.frame(hypothesis,
                         prior,
                         likelihood,
                         product,
                         posterior) |>
  add_row(hypothesis = "sum",
          prior = sum(prior),
          likelihood = NA,
          product = sum(product),
          posterior = sum(posterior))

kable(bayes_table, digits = 4, align = 'r')

```




## Solution to @exr-bayes-best-player

1.  Hypotheses are which player is best (A, B, C). 
Evidence is that A beats B. 
The likelihood is the probability that A beats B given each of the best players.

-   If A is best, probability A beats B is 2/3.
-   If B is best, probability A beats B is 1/3.
-   If C is best, probability A beats B is 1/2.

Compute the posterior probabilities as in the following Bayes table.

```{r}
#| echo: false

best_player = c("A", "B", "C")

prior = c(0.5, 0.35, 0.15)

likelihood_A_beats_B = c(2 / 3, 1 / 3, 1 / 2)

product = prior * likelihood_A_beats_B

posterior = product / sum(product)

bayes_table = data.frame(best_player,
                         prior,
                         likelihood_A_beats_B,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |> 
  kbl(digits = 4) |>
  kable_styling()

posterior_first = posterior

```

1.  A's probability of being the best increased, which makes sense because A won the match.
B's probability of being the best decreased considerably, which makes sense because B lost the match.
C's probability of being the best decreased slightly, despite C not being involved in the match.
(We have now observed some actual evidence in A's favor while we don't have any observations about C yet, and this information asymmetry results in a decrease in C's posterior probability.)


1.  Hypotheses are which player is best (A, B, C).
Evidence is that B beats A.
The likelihood is the probability that B beats A given each of the best players.

-   If A is best, probability B beats A is 1/3.
-   If B is best, probability B beats A is 2/3.
-   If C is best, probability B beats A is 1/2.

```{r}
#| echo: false

best_player = c("A", "B", "C")

prior = c(0.5, 0.35, 0.15)

likelihood_B_beats_A = c(1 / 3, 2 / 3, 1 / 2)

product = prior * likelihood_B_beats_A

posterior = product / sum(product)

bayes_table = data.frame(best_player,
                         prior,
                         likelihood_B_beats_A,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()

```

1.  A's probability of being the best decreased, which makes sense because A lost the match.
B's probability of being the best increased, which makes sense because B won the match.
C's probability of being the best changed slightly, despite C not being involved in the match.

1.  The prior is the posterior from the first part.
Evidence is that A beats C.
The likelihood is the probability that A beats C given each of the best players.

-   If A is best, probability A beats C is 2/3.
-   If B is best, probability A beats C is 1/2.
-   If C is best, probability A beats C is 1/3.

```{r}
#| echo: false

prior = posterior_first

likelihood_A_beats_C = c(2 / 3, 1 / 2, 1/ 3)

product = prior * likelihood_A_beats_C

posterior = product / sum(product)

bayes_table = data.frame(best_player,
                         prior,
                         likelihood_A_beats_C,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()

```

1.  By winning both matches, A's probability of being the best has increased considerably.
By losing their only matches, B's and C's probabilities of being the best have decreased considerably.



## Solution to @exr-bayes-best-player-predict

1.  You'd pick A and your subjective probability of being correct is 0.5.
1.  Use the law of total probability, conditioning on who is the best player (A, B, C)
\begin{align*}
\text{P}(\text{A beats B}) & = \text{P}(\text{A beats B}|A)\text{P}(A) + \text{P}(\text{A beats B}|B)\text{P}(B) + \text{P}(\text{A beats B}|C)\text{P}(C)\\
& = (2/3)(0.5) + (1/3)(0.35) + (1/2)(0.15) = 0.525
\end{align*}
1.  Given that A beats B, we would predict A to be the best player, and we would have probability 0.6349 of being correct.
1.  Given that B beats A, we would predict B to be the best player, and we would have probability 0.4912 of being correct.
1.  Use the law of total probability, conditioning on the result of the first match (A beats B or B beats A).
\begin{align*}
\text{P}(\text{correct}) & = \text{P}(\text{correct} | \text{A beats B})\text{P}(\text{A beats B}) + \text{P}(\text{correct} | \text{B beats A})  \text{P}(\text{B beats A})\\
& = (0.6349)(0.5250) + (0.4912)(0.4750) = 0.5666
\end{align*}
The information gained from observing the first match increases our probability of being correct from 0.5 to 0.5666.


### Solution to @exr-independent-geometric

1.  $X$ can take values 1, 2, 3, $\ldots$. 
Even though it is unlikely that $X$ is very large, there is no fixed upper bound. 
Even though $X$ can take infinitely many values, $X$ is a discrete random variables because it takes *countably* many possible values.
1.  $X= 1$ only if she makes her first attempt, so $\text{P}(X = 1) = 0.4$. If Maya does this every practice, then in about 40% of practices she will make her first three pointer on her first attempt.
1.  $X= 2$ only if she misses her first attempt and makes her second attempt, so since the attempts are independent $\text{P}(X = 2) = (0.6)(0.4)=0.24$. 
If Maya does this every practice, then in about 24% of practices she will make her first three pointer on her second attempt.
1.  In order for $X$ to be 3, Maya must miss her first two attempts and make her third. 
Since the attempts are independent $\text{P}(X=3)=(1-0.4)^2(0.4)=0.144$. 
If Maya does this every practice, then in about 14.4% of practices she will make her first three pointer on her third attempt.
1.  The key is to realize that Maya requires more than 3 attempts to obtain her first success if and only if the first 5 attempts are failures.
Therefore,
$$
P(X > 3) = (1-0.4)^3  = 0.216
$$



## Solution to @exr-branching-extinction



1.  Let $D$ be the probability that the original microorganism dies after the first minute; $\IP(D) = 1/4$.
Condition on the first "step" and use the law of total probability
$$
p = \IP(E) = \IP(E|D)\IP(D) + \IP(E|D^c)\IP(D^c) = (1)(1/4) + \IP(E|D^c)(3/4)
$$
$\IP(E|D) = 1$ since if the first microorganism dies the population goes extinct immediately.

    The key is to find an expression for $\IP(E|D^c)$ in terms of $p$. If the first microorganism does not die ($D^c$) there are 2 microorganisms at the start of the second minute; let's call them Marge and Homer. In order for the population to go extinct, we need Marge and all her descendants to go extinct, and the same for Homer. But Marge is just a single microorganism, so the probability that her line eventually goes extinct is $p$; similarly the probability that Homer's line goes extinct is $p$. Since all microorganisms behave independently, the probability that both Marge and Homer's lines eventually go extinct is $(p)(p)=p^2$. That is, $\IP(E | D^c) = p^2$.

    Plugging into the equation above yields
    $$
    p = (1)(1/4) + p^2(3/4)
    $$
    
    Solve (quadratic formula) this equation to get^[Technically, there are two solutions, 1 and $1/3$. There are some technical justifications that can be made to show that the extinction probability is the smaller of the two solutions, but this is beyond our scope.] $p= 1/3$. The probability that the population eventually goes extinct is 1/3. This microorganism population is 2 times more likely to survive forever than to go extinct!

1.  The process is the same as the above, with 3/4 replaced by $s$
$$
p = (1)(1-s) + p^2s
$$
Solving gives two solutions, 1 and $1/s - 1$.
However, if $s<1/2$ then $1/s - 1 > 1$, which is not a valid probability. Therefore the probability of eventual extinction is 1 if $s \le 1/2$, and $1/s - 1<1$ if $s > 1/2$.


## Solution to @exr-best-of-five

1.  It is helpful to construct a two-way table to answer the following questions.

    |                |                          |                                   |                                               |       |
    |:-------------|-------------:|-------------:|---------------:|-------------:|
    |                |                      *x* |                                   |                                               |       |
    |                |                        3 |                                 4 |                                             5 | Total |
    | A wins         | 0.55<sup>3</sup> = 0.166 | 3(0.55<sup>3</sup>)(0.45) = 0.225 | 6(0.55<sup>3</sup>)(0.45)<sup>2</sup> = 0.202 | 0.593 |
    | A does not win | 0.45<sup>3</sup> = 0.091 | 3(0.45<sup>3</sup>)(0.55) = 0.150 | 6(0.45<sup>3</sup>)(0.55)<sup>2</sup> = 0.165 | 0.407 |
    | Total          |                    0.258 |                             0.375 |                                         0.368 |     1 |

    There is only one outcome for which A wins in 3 games: AAA.
    There are three outcomes in which A wins in 4 games: AABA, ABAA, BAAA.
    (Not AAAB because then the series would be over in 3 games.) Since the games are independent, an outcome like AABA has probability $(0.55^3)(0.45)$, so the probability that A wins in 4 games is $3(0.55^3)(0.45)$.
    There are six outcomes in which A wins in 5 games: AABBA, ABABA, BAABA, ABBAA, BABAA, BBAAA.
    (Not outcomes like AAABB or AABAB because then the series would be over in 3 or 4 games.) Since the games are independent, an outcome like AABBA has probability $(0.55^3)(0.45)^2$, so the probability that A wins in 5 games is $6(0.55^3)(0.45)$.
    You can fill in the rest of the table similarly.

    The probability that team A wins the series in 3 games is $\text{P}(X=3, A) = 0.55^3=0.166$.

1.  Either the stronger team wins 3 in a row or loses 3 in a row.
    The probability is $0.55^3+(1-0.55)^3=0.2575$.

1.  The probability that team A wins the series is the sum of first row: $\text{P}(A) = 0.593$.

1.  No. $\text{P}(X=3, A) = 0.55^3=0.166 \neq 0.152 = (0.2575)(0.593) = \text{P}(X=3)\text{P}(A)$.
    Alternatively, $\text{P}(X = 3 | A) = 0.166/0.593 = 0.2799 \neq 0.2575 = \text{P}(X = 3)$.
    Given that $A$ wins the series the series it more likely to end in 3 games than when B wins the series.

1.  Total row above.
    $X$ can take values 3, 4, or 5.
    Consider first the ways in which the stronger team wins in 4 games: AABA, ABAA, BAAA.
    (For example AABA, means the stronger team wins game 1, 2, and 4, and the weaker team wins game 3).
    Each of these outcomes has probability $0.55^3(0.45)$ so the probability that team A wins in 4 games in $3(0.55)^3(0.45)$.
    Similarly, the probability that team B wins in 4 games is $3(0.45)^3(0.55)$.
    So
    $$
    \text{P}(X = 4) = 3(0.55)^3(0.45) + 3(0.45)^3(0.55) = 0.3750
    $$
    You could find $\text{P}(X=5)$ in a similar way, or just use the fact that the probabilities have to add up to 1.
    The distribution of $X$ is given by the following table.
    \begin{align*}
     x & \qquad & & \qquad \text{P}(X = x)\\
     3 & \qquad & & \qquad 0.2575\\
     4 & \qquad & & \qquad 0.3750\\
     5 & \qquad & & \qquad 0.3675\\
    \end{align*}



## Solution to @exr-bayes-best-player-iterate

1.  The prior is the posterior from the first part.
Evidence is that A beats C.
The likelihood is the probability that A beats C given each of the best players.

-   If A is best, probability A beats C is 2/3.
-   If B is best, probability A beats C is 1/2.
-   If C is best, probability A beats C is 1/3.

```{r}
#| echo: false

prior = posterior_first

likelihood_A_beats_C = c(2 / 3, 1 / 2, 1/ 3)

product = prior * likelihood_A_beats_C

posterior = product / sum(product)

bayes_table = data.frame(best_player,
                         prior,
                         likelihood_A_beats_C,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()

```


By winning both matches, A’s probability of being the best has increased considerably.
By losing their only matches, B’s and C’s probabilities of being the best have decreased considerably.

1.  The prior is the posterior from the previous part.
Evidence is that B beats C.
The likelihood is the probability that B beats C given each of the best players.

-   If A is best, probability B beats C is 1/2.
-   If B is best, probability B beats C is 2/3.
-   If C is best, probability B beats C is 1/3.

```{r}
#| echo: false

prior = posterior

likelihood_B_beats_C = c(1 / 2, 2 / 3, 1/ 3)

product = prior * likelihood_B_beats_C

posterior = product / sum(product)

bayes_table = data.frame(best_player,
                         prior,
                         likelihood_B_beats_C,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()

```


By winning both matches, A’s probability of being the best has increased considerably.
By losing one match and winning one, B’s probability of being the best decreased somewhat.
By losing both matches, C’s probability of being the best has decreased considerably.

1.  The prior is the original prior.
Evidence is that A beats B and A beats C and B beats C in three conditionally independent matches.
The likelihood is the probability of these match results given each of the best players.

-   If A is best, likelihood is (2/3)(2/3)(1/2).
-   If B is best, likelihood is (1/3)(1/2)(2/3).
-   If C is best, likelihood is (1/2)(1/3)(1/3)

```{r}
#| echo: false


prior = c(0.5, 0.35, 0.15)

likelihood = likelihood_A_beats_B *
  likelihood_A_beats_C *
  likelihood_B_beats_C

product = prior * likelihood

posterior = product / sum(product)

bayes_table = data.frame(best_player,
                         prior,
                         likelihood,
                         product,
                         posterior)

bayes_table |>
  adorn_totals("row") |>
  kbl(digits = 4) |>
  kable_styling()

```

The posterior is the same.
It doesn't matter if the posterior is updated after each match, or at once after all three matches.




